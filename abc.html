<!DOCTYPE html>
<html>
<head>
    <title>Hypothesis Generation</title>
    <style>
        body {
            background-color: black;
            color: white;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .code-box {
            background-color: #222;
            padding: 10px;
            margin-bottom: 20px;
        }
        .code {
            font-family: monospace;
            color: white;
            white-space: pre;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Hypothesis Generation</h1>

        <div class="code-box">
            <h3>The Find-S Inductive algorithm.</h3>
            <h4>Code 1</h4>
            <div class="code">
                <code>
import pandas as pd
import numpy as np

# To read the data in the CSV file
data = pd.read_csv("hdata.csv")
print(data, "n")

# Making an array of all the attributes
d = np.array(data)[:,:-1]
print("n The attributes are: ", d)

# Segregating the target that has positive and negative examples
target = np.array(data)[:,-1]
print("n The target is: ", target)

# Training function to implement find-s algorithm
def train(c, t):
    for i, val in enumerate(t):
        if val == "Yes":
            specific_hypothesis = c[i].copy()
            break

    for i, val in enumerate(c):
        if t[i] == "Yes":
            for x in range(len(specific_hypothesis)):
                if val[x] != specific_hypothesis[x]:
                    specific_hypothesis[x] = '?'
                else:
                    pass

    return specific_hypothesis

# Obtaining the final hypothesis
print("n The final hypothesis is:", train(d, target))
                </code>
            </div>
        </div>

        <div class="code-box">
            <h3>The Candidate-Elimination Inductive Learining algorithm.</h3>
            <h4>Code 2</h4>
            <div class="code">
                <code>
import numpy as np
import pandas as pd

data = pd.read_csv('h2data.csv')
concepts = np.array(data.iloc[:,0:-1])
print("\nInstances are:\n", concepts)
target = np.array(data.iloc[:,-1])
print("\nTarget Values are: ", target)

def learn(concepts, target):
    specific_h = concepts[0].copy()
    print("\nInitialization of specific_h and general_h")
    print("\nSpecific Boundary: ", specific_h)
    general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    print("\nGeneric Boundary: ", general_h)

    for i, h in enumerate(concepts):
        print("\nInstance", i+1, "is ", h)
        if target[i] == "yes":
            print("Instance is Positive ")
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    specific_h[x] ='?'
                    general_h[x][x] ='?'

        if target[i] == "no":
            print("Instance is Negative ")
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    general_h[x][x] = specific_h[x]
                else:
                    general_h[x][x] = '?'

        print("Specific Boundary after ", i+1, "Instance is ", specific_h)
        print("Generic Boundary after ", i+1, "Instance is ", general_h)
        print("\n")

    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]
    for i in indices:
        general_h.remove(['?', '?', '?', '?', '?', '?'])
    return specific_h, general_h

s_final, g_final = learn(concepts, target)

print("Final Specific_h: ", s_final, sep="\n")
print("Final General_h: ", g_final, sep="\n")
                </code>
            </div>
        </div>
        <div class="code-box">
        <h3>a program to implement Decision tree using </h3>
            <h4>Code 3</h4>
            <div class="code">
                <code>
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

col_names=['pregnant','glucose','bp','shin','insulin','bmi','pedigree','age','label']
pima = pd.read_csv("pima-indians-diabetes.csv", header = None, names = col_names)
pima.head( )
pima.sample( )
pima.tail( )
#split dataset in feature and target variable
feature_cols=['pregnant','insulin','bmi','age','glucose','bp','pedigree']
X=pima[feature_cols]# Feature
Y=pima.label # target variable

# split dataset inti training set and test set
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3,random_state=1)# 70% training and 30% testing

# crete decision tree classifier object
clf = DecisionTreeClassifier()

# train decision tree classifier
clf =clf.fit(X_train,Y_train)

# predict the response for the test dataset
Y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, Y_pred))
</code>
</div>
</div>
         <div class="code-box">
        <h3>write program to calculate popular attribute selection measures(ASM)like Information Gain, GainRatio , and Gini Index etc.for decision tree</h3>
            <h4>Code 4</h4>
            <div class="code">
                <code>
# Load libraries
import pandas as pd
# Import Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
                    
#Import train_test_split function
from sklearn.model_selection import train_test_split 
                    
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
                    
col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']
                    
# load dataset
pima = pd.read_csv("pima-indians-diabetes.csv", header=None, names=col_names)
pima.head()
                    
# Split dataset in features and target variable
feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']
                    
X = pima[feature_cols] # Features
y = pima.label # Target variable
                    
# Split the dataset into the training set and test set
                    
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) 
# 70% training and 30% test
                    
# Create Decision Tree classifer object
clf = DecisionTreeClassifier()
                    
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
                    
#Predict the response for test dataset
y_pred = clf.predict(X_test)
                    
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
                    
from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus
                    
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes.png')
Image(graph.create_png())
                    
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion="entropy", max_depth=3)
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
                    
#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
                </code>
            </div>
         </div>
        <div class="code-box">
        <h3>Implement simple KNN using Euclidean distance in python</h3>
            <h4>Code 5</h4>
            <div class="code">
                <code>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1.5, random_state=4)

plt.style.use('seaborn')
plt.figure(figsize=(10, 10))
plt.scatter(X[:, 0], X[:, 1], c=y, marker='*', s=100, edgecolors='black')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

knn5 = KNeighborsClassifier(n_neighbors=5)
knn1 = KNeighborsClassifier(n_neighbors=1)

knn5.fit(X_train, y_train)
knn1.fit(X_train, y_train)

y_pred_5 = knn5.predict(X_test)
y_pred_1 = knn1.predict(X_test)

from sklearn.metrics import accuracy_score

print("Accuracy with k=5:", accuracy_score(y_test, y_pred_5) * 100)
print("Accuracy with k=1:", accuracy_score(y_test, y_pred_1) * 100) 
                </code>
            </div>
        </div>
        <div class="code-box">
        <h3>Write a program to implement k-Nearest Neighbour algorithm to classify 
         the iris dataset. Print both correct and wrong predictions library classes can be used for this problem.</h3>
            <h4>Code 6</h4>
            <div class="code">
                <code>
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

# Read dataset to pandas dataframe
dataset = pd.read_csv("8-dataset.csv", names=names)
X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]

print(X.head())

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.10)
classifier = KNeighborsClassifier(n_neighbors=5).fit(Xtrain, ytrain)
ypred = classifier.predict(Xtest)

i = 0
print("\n-------------------------------------------------------------------------")
print('%-25s %-25s %-25s' % ('Original Label', 'Predicted Label', 'Correct/Wrong'))
print("-------------------------------------------------------------------------")
for label in ytest:
    print('%-25s %-25s' % (label, ypred[i]), end="")
    if (label == ypred[i]):
        print(' %-25s' % ('Correct'))
    else:
        print(' %-25s' % ('Wrong'))
    i = i + 1

print("-------------------------------------------------------------------------")
print("\nConfusion Matrix:\n", metrics.confusion_matrix(ytest, ypred))
print("-------------------------------------------------------------------------")
print("\nClassification Report:\n", metrics.classification_report(ytest, ypred))
print("-------------------------------------------------------------------------")
print('Accuracy of the classifier is %0.2f' % metrics.accuracy_score(ytest, ypred))
print("-------------------------------------------------------------------------")   
                </code>
            </div>
        </div>
         <div class="code-box">
        <h3>Write a program to implement the na√Øve Bayesian classifier for a sample 
        training dataset stored as a .CSV file. Compute the accuracy of the classifier, 
        considering few test data sets</h3>
            <h4>Code 7</h4>
            <div class="code">
                <code>
 # Import important libraries
import numpy as np
import pandas as pd
# Import dataset
from sklearn import datasets
# Load dataset
wine = datasets.load_wine()
# print(wine)  # if you want to see the data you can print data
# print the names of the 13 features
print("Features: ", wine.feature_names)
# print the label type of wine
print("Labels: ", wine.target_names)

X = pd.DataFrame(wine['data'])
print(X.head())
print(wine.data.shape)

# print the wine labels (0:Class_0, 1:class_2, 2:class_2)
y = print(wine.target)

# Import train_test_split function
from sklearn.model_selection import train_test_split
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target,
                                                    test_size=0.30, random_state=109)

# Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB
# Create a Gaussian Classifier
gnb = GaussianNB()
# Train the model using the training sets
gnb.fit(X_train, y_train)
# Predict the response for test dataset
y_pred = gnb.predict(X_test)
print(y_pred)

# Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))

# Confusion matrix
from sklearn.metrics import confusion_matrix
cm = np.array(confusion_matrix(y_test, y_pred) ,cm )
                </code>
            </div>
         </div>
        <div class="code-box">
        <h3>Write a Program for Confusion Matrix and calculate Precision, Recall, F-
         Measure.</h3>
            <h4>Code 8</h4>
            <div class="code">
                <code>
# Import the necessary libraries
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Create the NumPy array for actual and predicted labels.
actual = np.array(['Dog', 'Dog', 'Dog', 'Not Dog', 'Dog', 'Not Dog', 'Dog', 'Dog', 'Not Dog', 'Not Dog'])
predicted = np.array(['Dog', 'Not Dog', 'Dog', 'Not Dog', 'Dog', 'Dog', 'Dog', 'Dog', 'Not Dog', 'Not Dog'])

# Compute the confusion matrix.
cm = confusion_matrix(actual, predicted)

# Plot the confusion matrix.
sns.heatmap(cm, annot=True, fmt='g', xticklabels=['Dog', 'Not Dog'], yticklabels=['Dog', 'Not Dog'])
plt.ylabel('Prediction', fontsize=13)
plt.xlabel('Actual', fontsize=13)
plt.title('Confusion Matrix', fontsize=17)
plt.show() 
                </code>
            </div>
        </div>
         <div class="code-box">
        <h3>Write program for linear regression and find parameters like Sum of 
         Squared Errors (SSE), Total Sum of Squares (SST), R2, Adjusted R2etc.</h3>
            <h4>Code 9</h4>
            <div class="code">
                <code>
import pandas as pd
import statsmodels.api as sm
import numpy as np

df = pd.DataFrame({
    'hours': (1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 7, 7, 8),
    'score': (68, 76, 74, 80, 76, 78, 81, 84, 86, 83, 88, 85, 89, 94, 93, 94, 96, 89, 92, 97)
})

# View first six rows of the data frame
df.head()

# Define response variable
y = df['score']

# Define predictor variable
x = df[['hours']]

# Add constant to predictor variables
x = sm.add_constant(x)

# Fit linear regression model
model = sm.OLS(y, x).fit()

# Calculate SSE
sse = np.sum((model.fittedvalues - df.score) ** 2)
print(sse)

# Calculate SSR
ssr = np.sum((model.fittedvalues - df.score.mean()) ** 2)
print(ssr)

# Calculate SST
sst = ssr + sse
print(sst)
                </code>
            </div>
         </div>
         <div class="code-box">
        <h3>Implementing Agglomerative Clustering in python.</h3>
            <h4>Code 10</h4>
            <div class="code">
                <code>
   import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.metrics import silhouette_score
import scipy.cluster.hierarchy as shc

# Changing the working location to the location of the file
X = pd.read_csv('CC GENERAL.csv')

# Dropping the CUST_ID column from the data
X = X.drop('CUST_ID', axis=1)

# Handling the missing values
X.fillna(method='ffill', inplace=True)

# Scaling the data so that all the features become comparable
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Normalizing the data so that the data approximately follows a Gaussian distribution
X_normalized = normalize(X_scaled)

# Converting the numpy array into a pandas DataFrame
X_normalized = pd.DataFrame(X_normalized)

pca = PCA(n_components=2)
X_principal = pca.fit_transform(X_normalized)
X_principal = pd.DataFrame(X_principal)
X_principal.columns = ['P1', 'P2']

plt.figure(figsize=(8, 8))
plt.title('Visualising the data')
Dendrogram = shc.dendrogram((shc.linkage(X_principal, method='ward')))
ac2 = AgglomerativeClustering(n_clusters = 2)
# Visualizing the clustering
plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
c = ac2.fit_predict(X_principal), cmap ='rainbow')
plt.show()

ac3 = AgglomerativeClustering(n_clusters = 3)
plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
c = ac3.fit_predict(X_principal), cmap ='rainbow')
plt.show()

ac4 = AgglomerativeClustering(n_clusters = 4)
plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
c = ac4.fit_predict(X_principal), cmap ='rainbow')
plt.show()

ac5 = AgglomerativeClustering(n_clusters = 5)
plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
c = ac5.fit_predict(X_principal), cmap ='rainbow')
plt.show()

ac6 = AgglomerativeClustering(n_clusters = 6)
plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
c = ac6.fit_predict(X_principal), cmap ='rainbow')
plt.show()

k = [2, 3, 4, 5, 6]
# Appending the silhouette scores of the different models to the list
silhouette_scores = []
silhouette_scores.append(
silhouette_score(X_principal, ac2.fit_predict(X_principal)))
silhouette_scores.append(
silhouette_score(X_principal, ac3.fit_predict(X_principal)))
silhouette_scores.append(
silhouette_score(X_principal, ac4.fit_predict(X_principal)))
silhouette_scores.append(
silhouette_score(X_principal, ac5.fit_predict(X_principal)))
silhouette_scores.append(silhouette_score(X_principal, ac6.fit_predict(X_principal)))

# Plotting a bar graph to compare the results
plt.bar(k, silhouette_scores)
plt.xlabel('Number of clusters', fontsize = 20)
plt.ylabel('S(i)', fontsize = 20)
plt.show()                    
                </code>
            </div>
         </div>
                 <div class="code-box">
        <h3>.Write a program for Fuzzy c-means clustering in python</h3>
            <h4>Code 11</h4>
            <div class="code">
                <code>
import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl

# Generate some example data
np.random.seed(0)
data = np.random.rand(100, 2)

# Define the number of clusters
n_clusters = 3

# Apply fuzzy c-means clustering
cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(data.T, n_clusters, 2, error=0.005, maxiter=1000, init=None)

# Predict cluster membership for each data point
cluster_membership = np.argmax(u, axis=0)

# Print the cluster centers
print('Cluster Centers:', cntr)

# Print the cluster membership for each data point
print('Cluster Membership:', cluster_membership)  
                </code>
            </div>
         </div>
        <div class="code-box">
        <h3>Implement the non-parametric Locally Weighted Regression algorithm in 
            order to fit data points. Select appropriate data set for your experiment and 
            draw graphs.</h3>
            <h4>Code 12</h4>
            <div class="code">
                <code>
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def kernel(point, xmat, k):
    m, n = np.shape(xmat)
    weights = np.mat(np.eye(m))
    
    for j in range(m):
        diff = point - xmat[j]
        weights[j, j] = np.exp(diff * diff.T / (-2.0 * k ** 2))
    
    return weights

def localWeight(point, xmat, ymat, k):
    wei = kernel(point, xmat, k)
    W = (xmat.T * (wei * xmat)).I * (xmat.T * (wei * ymat.T))
    
    return W

def localWeightRegression(xmat, ymat, k):
    m, n = np.shape(xmat)
    ypred = np.zeros(m)
    
    for i in range(m):
        ypred[i] = xmat[i] * localWeight(xmat[i], xmat, ymat, k)
    
    return ypred

# Load data points
data = pd.read_csv('10-dataset.csv')
bill = np.array(data.total_bill)
tip = np.array(data.tip)

# Preparing and add 1 in bill
mbill = np.mat(bill)
mtip = np.mat(tip)
m = np.shape(mbill)[1]
one = np.mat(np.ones(m))
X = np.hstack((one.T, mbill.T))

# Set k here
ypred = localWeightRegression(X, mtip, 0.5)
SortIndex = X[:, 1].argsort(0)
xsort = X[SortIndex][:, 0]

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
ax.scatter(bill, tip, color='green')
ax.plot(xsort[:, 1], ypred[SortIndex], color='red', linewidth=5)

plt.xlabel('Total bill')
plt.ylabel('Tip')
plt.show()
                </code>
            </div>
        </div>
         <div class="code-box">
        <h3>Build an Artificial Neural Network by implementing the 
        Backpropagation algorithm and test the same using appropriate data sets</h3>
            <h4>Code 13</h4>
            <div class="code">
                <code>
# Import Libraries
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load dataset
data = load_iris()
# Get features and target
X = data.data
y = data.target

# Get dummy variable
y = pd.get_dummies(y).values
y[:3]

# Split data into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=4)

# Initialize variables
learning_rate = 0.1
iterations = 5000
N = y_train.size

# Number of input features
input_size = 4
# Number of hidden layer neurons
hidden_size = 2
# Number of neurons at the output layer
output_size = 3

results = pd.DataFrame(columns=["mse", "accuracy"])

# Initialize weights
np.random.seed(10)
# Initializing weight for the hidden layer
W1 = np.random.normal(scale=0.5, size=(input_size, hidden_size))
# Initializing weight for the output layer
W2 = np.random.normal(scale=0.5, size=(hidden_size, output_size))


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def mean_squared_error(y_pred, y_true):
    return ((y_pred - y_true) ** 2).sum() / (2 * y_pred.size)


def accuracy(y_pred, y_true):
    acc = y_pred.argmax(axis=1) == y_true.argmax(axis=1)
    return acc.mean()


for itr in range(iterations):
    # Feedforward propagation
    # On hidden layer
    Z1 = np.dot(X_train, W1)
    A1 = sigmoid(Z1)
    # On output layer
    Z2 = np.dot(A1, W2)
    A2 = sigmoid(Z2)

    # Calculating error
    mse = mean_squared_error(A2, y_train)
    acc = accuracy(A2, y_train)
    results = results.append({"mse": mse, "accuracy": acc}, ignore_index=True)

    # Backpropagation
    E1 = A2 - y_train
    dW1 = E1 * A2 * (1 - A2)
    E2 = np.dot(dW1, W2.T)
    dW2 = E2 * A1 * (1 - A1)

    # Weight updates
    W2_update = np.dot(A1.T, dW1) / N
    W1_update = np.dot(X_train.T, dW2) / N
    W2 = W2 - learning_rate * W2_update
    W1 = W1 - learning_rate * W1_update

results.mse.plot(title="Mean Squared Error")
results.accuracy.plot(title="Accuracy") 
                </code>
            </div>
         </div>
        
 </div>
</body>
</html>
