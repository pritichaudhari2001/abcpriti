<!DOCTYPE html>
<html>
<head>
    <title>Hypothesis Generation</title>
    <style>
        body {
            background-color: black;
            color: white;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .code-box {
            background-color: #222;
            padding: 10px;
            margin-bottom: 20px;
        }
        .code {
            font-family: monospace;
            color: white;
            white-space: pre;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Hypothesis Generation</h1>

        <div class="code-box">
            <h3>The Find-S Inductive algorithm.</h3>
            <h4>Code 1</h4>
            <div class="code">
                <code>
import pandas as pd
import numpy as np

# To read the data in the CSV file
data = pd.read_csv("hdata.csv")
print(data, "n")

# Making an array of all the attributes
d = np.array(data)[:,:-1]
print("n The attributes are: ", d)

# Segregating the target that has positive and negative examples
target = np.array(data)[:,-1]
print("n The target is: ", target)

# Training function to implement find-s algorithm
def train(c, t):
    for i, val in enumerate(t):
        if val == "Yes":
            specific_hypothesis = c[i].copy()
            break

    for i, val in enumerate(c):
        if t[i] == "Yes":
            for x in range(len(specific_hypothesis)):
                if val[x] != specific_hypothesis[x]:
                    specific_hypothesis[x] = '?'
                else:
                    pass

    return specific_hypothesis

# Obtaining the final hypothesis
print("n The final hypothesis is:", train(d, target))
                </code>
            </div>
        </div>

        <div class="code-box">
            <h3>The Candidate-Elimination Inductive Learining algorithm.</h3>
            <h4>Code 2</h4>
            <div class="code">
                <code>
import numpy as np
import pandas as pd

data = pd.read_csv('h2data.csv')
concepts = np.array(data.iloc[:,0:-1])
print("\nInstances are:\n", concepts)
target = np.array(data.iloc[:,-1])
print("\nTarget Values are: ", target)

def learn(concepts, target):
    specific_h = concepts[0].copy()
    print("\nInitialization of specific_h and general_h")
    print("\nSpecific Boundary: ", specific_h)
    general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    print("\nGeneric Boundary: ", general_h)

    for i, h in enumerate(concepts):
        print("\nInstance", i+1, "is ", h)
        if target[i] == "yes":
            print("Instance is Positive ")
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    specific_h[x] ='?'
                    general_h[x][x] ='?'

        if target[i] == "no":
            print("Instance is Negative ")
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    general_h[x][x] = specific_h[x]
                else:
                    general_h[x][x] = '?'

        print("Specific Boundary after ", i+1, "Instance is ", specific_h)
        print("Generic Boundary after ", i+1, "Instance is ", general_h)
        print("\n")

    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]
    for i in indices:
        general_h.remove(['?', '?', '?', '?', '?', '?'])
    return specific_h, general_h

s_final, g_final = learn(concepts, target)

print("Final Specific_h: ", s_final, sep="\n")
print("Final General_h: ", g_final, sep="\n")
                </code>
            </div>
        </div>
        <div class="code-box">
        <h3>a program to implement Decision tree using </h3>
            <h4>Code 3</h4>
            <div class="code">
                <code>
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

col_names=['pregnant','glucose','bp','shin','insulin','bmi','pedigree','age','label']
pima = pd.read_csv("pima-indians-diabetes.csv", header = None, names = col_names)
pima.head( )
pima.sample( )
pima.tail( )
#split dataset in feature and target variable
feature_cols=['pregnant','insulin','bmi','age','glucose','bp','pedigree']
X=pima[feature_cols]# Feature
Y=pima.label # target variable

# split dataset inti training set and test set
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3,random_state=1)# 70% training and 30% testing

# crete decision tree classifier object
clf = DecisionTreeClassifier()

# train decision tree classifier
clf =clf.fit(X_train,Y_train)

# predict the response for the test dataset
Y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, Y_pred))
</code>
</div>
</div>
         <div class="code-box">
        <h3>write program to calculate popular attribute selection measures(ASM)like Information Gain, GainRatio , and Gini Index etc.for decision tree</h3>
            <h4>Code 4</h4>
            <div class="code">
                <code>
# Load libraries
import pandas as pd
# Import Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
                    
#Import train_test_split function
from sklearn.model_selection import train_test_split 
                    
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
                    
col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']
                    
# load dataset
pima = pd.read_csv("pima-indians-diabetes.csv", header=None, names=col_names)
pima.head()
                    
# Split dataset in features and target variable
feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']
                    
X = pima[feature_cols] # Features
y = pima.label # Target variable
                    
# Split the dataset into the training set and test set
                    
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) 
# 70% training and 30% test
                    
# Create Decision Tree classifer object
clf = DecisionTreeClassifier()
                    
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
                    
#Predict the response for test dataset
y_pred = clf.predict(X_test)
                    
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
                    
from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus
                    
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes.png')
Image(graph.create_png())
                    
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion="entropy", max_depth=3)
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
                    
#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
                </code>
            </div>
         </div>
        <div class="code-box">
        <h3>Implement simple KNN using Euclidean distance in python</h3>
            <h4>Code 5</h4>
            <div class="code">
                <code>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1.5, random_state=4)

plt.style.use('seaborn')
plt.figure(figsize=(10, 10))
plt.scatter(X[:, 0], X[:, 1], c=y, marker='*', s=100, edgecolors='black')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

knn5 = KNeighborsClassifier(n_neighbors=5)
knn1 = KNeighborsClassifier(n_neighbors=1)

knn5.fit(X_train, y_train)
knn1.fit(X_train, y_train)

y_pred_5 = knn5.predict(X_test)
y_pred_1 = knn1.predict(X_test)

from sklearn.metrics import accuracy_score

print("Accuracy with k=5:", accuracy_score(y_test, y_pred_5) * 100)
print("Accuracy with k=1:", accuracy_score(y_test, y_pred_1) * 100) 
                </code>
            </div>
        </div>
        <div class="code-box">
        <h3>Write a program to implement k-Nearest Neighbour algorithm to classify 
         the iris dataset. Print both correct and wrong predictions library classes can be used for this problem.</h3>
            <h4>Code 6</h4>
            <div class="code">
                <code>
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

# Read dataset to pandas dataframe
dataset = pd.read_csv("8-dataset.csv", names=names)
X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]

print(X.head())

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.10)
classifier = KNeighborsClassifier(n_neighbors=5).fit(Xtrain, ytrain)
ypred = classifier.predict(Xtest)

i = 0
print("\n-------------------------------------------------------------------------")
print('%-25s %-25s %-25s' % ('Original Label', 'Predicted Label', 'Correct/Wrong'))
print("-------------------------------------------------------------------------")
for label in ytest:
    print('%-25s %-25s' % (label, ypred[i]), end="")
    if (label == ypred[i]):
        print(' %-25s' % ('Correct'))
    else:
        print(' %-25s' % ('Wrong'))
    i = i + 1

print("-------------------------------------------------------------------------")
print("\nConfusion Matrix:\n", metrics.confusion_matrix(ytest, ypred))
print("-------------------------------------------------------------------------")
print("\nClassification Report:\n", metrics.classification_report(ytest, ypred))
print("-------------------------------------------------------------------------")
print('Accuracy of the classifier is %0.2f' % metrics.accuracy_score(ytest, ypred))
print("-------------------------------------------------------------------------")   
                </code>
            </div>
        </div>
         <div class="code-box">
        <h3>Write a program to implement the na√Øve Bayesian classifier for a sample 
        training dataset stored as a .CSV file. Compute the accuracy of the classifier, 
        considering few test data sets</h3>
            <h4>Code 7</h4>
            <div class="code">
                <code>
 # Import important libraries
import numpy as np
import pandas as pd
# Import dataset
from sklearn import datasets
# Load dataset
wine = datasets.load_wine()
# print(wine)  # if you want to see the data you can print data
# print the names of the 13 features
print("Features: ", wine.feature_names)
# print the label type of wine
print("Labels: ", wine.target_names)

X = pd.DataFrame(wine['data'])
print(X.head())
print(wine.data.shape)

# print the wine labels (0:Class_0, 1:class_2, 2:class_2)
y = print(wine.target)

# Import train_test_split function
from sklearn.model_selection import train_test_split
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target,
                                                    test_size=0.30, random_state=109)

# Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB
# Create a Gaussian Classifier
gnb = GaussianNB()
# Train the model using the training sets
gnb.fit(X_train, y_train)
# Predict the response for test dataset
y_pred = gnb.predict(X_test)
print(y_pred)

# Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))

# Confusion matrix
from sklearn.metrics import confusion_matrix
cm = np.array(confusion_matrix(y_test, y_pred) ,cm )
                </code>
            </div>
         </div>
        <div class="code-box">
        <h3>Write a Program for Confusion Matrix and calculate Precision, Recall, F-
         Measure.</h3>
            <h4>Code 8</h4>
            <div class="code">
                <code>
# Import the necessary libraries
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Create the NumPy array for actual and predicted labels.
actual = np.array(['Dog', 'Dog', 'Dog', 'Not Dog', 'Dog', 'Not Dog', 'Dog', 'Dog', 'Not Dog', 'Not Dog'])
predicted = np.array(['Dog', 'Not Dog', 'Dog', 'Not Dog', 'Dog', 'Dog', 'Dog', 'Dog', 'Not Dog', 'Not Dog'])

# Compute the confusion matrix.
cm = confusion_matrix(actual, predicted)

# Plot the confusion matrix.
sns.heatmap(cm, annot=True, fmt='g', xticklabels=['Dog', 'Not Dog'], yticklabels=['Dog', 'Not Dog'])
plt.ylabel('Prediction', fontsize=13)
plt.xlabel('Actual', fontsize=13)
plt.title('Confusion Matrix', fontsize=17)
plt.show() 
                </code>
            </div>
        </div>
         <div class="code-box">
        <h3>Write program for linear regression and find parameters like Sum of 
         Squared Errors (SSE), Total Sum of Squares (SST), R2, Adjusted R2etc.</h3>
            <h4>Code 9</h4>
            <div class="code">
                <code>
import pandas as pd
import statsmodels.api as sm
import numpy as np

df = pd.DataFrame({
    'hours': (1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 7, 7, 8),
    'score': (68, 76, 74, 80, 76, 78, 81, 84, 86, 83, 88, 85, 89, 94, 93, 94, 96, 89, 92, 97)
})

# View first six rows of the data frame
df.head()

# Define response variable
y = df['score']

# Define predictor variable
x = df[['hours']]

# Add constant to predictor variables
x = sm.add_constant(x)

# Fit linear regression model
model = sm.OLS(y, x).fit()

# Calculate SSE
sse = np.sum((model.fittedvalues - df.score) ** 2)
print(sse)

# Calculate SSR
ssr = np.sum((model.fittedvalues - df.score.mean()) ** 2)
print(ssr)

# Calculate SST
sst = ssr + sse
print(sst)
                </code>
            </div>
         </div>
    
                
              
      </div>
</body>
</html>
